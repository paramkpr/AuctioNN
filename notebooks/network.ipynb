{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Development Playground\n",
    "To use this notebook, you need to have the \"clean_data.parquet\" file in the ./data/ directory. I will share this file with you on Google Drive. However, if you want to create it on your own, you will need to:\n",
    "```\n",
    "1. Download the impressions and conversions data from Claritas â€“ Use the aws cli commands in the README.md file. (Credentials are in GH secrets)\n",
    "2. Run `python main.py preprocess --impressions-file ./data/impressions.csv --conversions-file ./data/conversions.csv --output-file ./data/clean_data.parquet`\n",
    "     - This command will clean the data for you and save it as a parquet file. It's pretty cool, actually, because I've made it use multiprocessing to speed up the user agent string parsing. \n",
    "```\n",
    "### Goal For this Notebook\n",
    "I want to use the `src.data_processing.datasets.AuctionDataset` class to create a PyTorch Dataset and develop the required functionality for this class. Then I want to use this dataset to train a simple neural network.\n",
    "\n",
    "### Dataset Overview\n",
    "Each dataset row contains information about a particular impression, and it contains a `conversion_flag` column that indicates whether the impression resulted in a conversion. We're trying to predict the `conversion_flag` column given the other information about the impression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized. Number of samples: 3845798\n",
      "Number of features: 16\n",
      "Feature names: ['placement_id', 'cnxn_type', 'dma', 'country', 'prizm_premier_code', 'campaign_id', 'ua_browser', 'ua_os', 'ua_device_family', 'ua_device_brand', 'ua_is_mobile', 'ua_is_tablet', 'ua_is_pc', 'ua_is_bot', 'impression_hour', 'impression_dayofweek']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.data_processing.datasets import AuctionDataset\n",
    "\n",
    "dataset = AuctionDataset(dataframe=pd.read_parquet('./data/clean_data.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3845798 entries, 0 to 3845797\n",
      "Data columns (total 16 columns):\n",
      " #   Column                Dtype \n",
      "---  ------                ----- \n",
      " 0   placement_id          int64 \n",
      " 1   cnxn_type             object\n",
      " 2   dma                   int32 \n",
      " 3   country               object\n",
      " 4   prizm_premier_code    object\n",
      " 5   campaign_id           int32 \n",
      " 6   ua_browser            object\n",
      " 7   ua_os                 object\n",
      " 8   ua_device_family      object\n",
      " 9   ua_device_brand       object\n",
      " 10  ua_is_mobile          bool  \n",
      " 11  ua_is_tablet          bool  \n",
      " 12  ua_is_pc              bool  \n",
      " 13  ua_is_bot             bool  \n",
      " 14  impression_hour       int32 \n",
      " 15  impression_dayofweek  int32 \n",
      "dtypes: bool(4), int32(4), int64(1), object(7)\n",
      "memory usage: 308.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placement_id</th>\n",
       "      <th>cnxn_type</th>\n",
       "      <th>dma</th>\n",
       "      <th>country</th>\n",
       "      <th>prizm_premier_code</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>ua_browser</th>\n",
       "      <th>ua_os</th>\n",
       "      <th>ua_device_family</th>\n",
       "      <th>ua_device_brand</th>\n",
       "      <th>ua_is_mobile</th>\n",
       "      <th>ua_is_tablet</th>\n",
       "      <th>ua_is_pc</th>\n",
       "      <th>ua_is_bot</th>\n",
       "      <th>impression_hour</th>\n",
       "      <th>impression_dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.845798e+06</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3.845798e+06</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798.0</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3845798</td>\n",
       "      <td>3.845798e+06</td>\n",
       "      <td>3.845798e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121</td>\n",
       "      <td>17</td>\n",
       "      <td>914</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Cable/DSL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>us</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Podcasts</td>\n",
       "      <td>iOS</td>\n",
       "      <td>iOS-Device</td>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2094122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3843051</td>\n",
       "      <td>2428584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1720968</td>\n",
       "      <td>2641499</td>\n",
       "      <td>1867796</td>\n",
       "      <td>2647091</td>\n",
       "      <td>3186944</td>\n",
       "      <td>3718120</td>\n",
       "      <td>3755881</td>\n",
       "      <td>3797739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.902147e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.883996e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.165020e+01</td>\n",
       "      <td>2.886486e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.461252e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.641265e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.868596e+00</td>\n",
       "      <td>1.977513e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.576500e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.967710e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.020000e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.967720e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.020000e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.967720e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.020000e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.967720e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.810000e+02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        placement_id  cnxn_type           dma  country prizm_premier_code  \\\n",
       "count   3.845798e+06    3845798  3.845798e+06  3845798            3845798   \n",
       "unique           NaN          3           NaN       47                 69   \n",
       "top              NaN  Cable/DSL           NaN       us            Unknown   \n",
       "freq             NaN    2094122           NaN  3843051            2428584   \n",
       "mean    5.902147e+05        NaN  5.883996e+02      NaN                NaN   \n",
       "std     1.461252e+04        NaN  8.641265e+01      NaN                NaN   \n",
       "min     5.576500e+05        NaN  0.000000e+00      NaN                NaN   \n",
       "25%     5.967710e+05        NaN  6.020000e+02      NaN                NaN   \n",
       "50%     5.967720e+05        NaN  6.020000e+02      NaN                NaN   \n",
       "75%     5.967720e+05        NaN  6.020000e+02      NaN                NaN   \n",
       "max     5.967720e+05        NaN  8.810000e+02      NaN                NaN   \n",
       "\n",
       "        campaign_id ua_browser    ua_os ua_device_family ua_device_brand  \\\n",
       "count     3845798.0    3845798  3845798          3845798         3845798   \n",
       "unique          NaN        121       17              914              29   \n",
       "top             NaN   Podcasts      iOS       iOS-Device           Apple   \n",
       "freq            NaN    1720968  2641499          1867796         2647091   \n",
       "mean         9317.0        NaN      NaN              NaN             NaN   \n",
       "std             0.0        NaN      NaN              NaN             NaN   \n",
       "min          9317.0        NaN      NaN              NaN             NaN   \n",
       "25%          9317.0        NaN      NaN              NaN             NaN   \n",
       "50%          9317.0        NaN      NaN              NaN             NaN   \n",
       "75%          9317.0        NaN      NaN              NaN             NaN   \n",
       "max          9317.0        NaN      NaN              NaN             NaN   \n",
       "\n",
       "       ua_is_mobile ua_is_tablet ua_is_pc ua_is_bot  impression_hour  \\\n",
       "count       3845798      3845798  3845798   3845798     3.845798e+06   \n",
       "unique            2            2        2         2              NaN   \n",
       "top            True        False    False     False              NaN   \n",
       "freq        3186944      3718120  3755881   3797739              NaN   \n",
       "mean            NaN          NaN      NaN       NaN     1.165020e+01   \n",
       "std             NaN          NaN      NaN       NaN     6.868596e+00   \n",
       "min             NaN          NaN      NaN       NaN     0.000000e+00   \n",
       "25%             NaN          NaN      NaN       NaN     6.000000e+00   \n",
       "50%             NaN          NaN      NaN       NaN     1.200000e+01   \n",
       "75%             NaN          NaN      NaN       NaN     1.800000e+01   \n",
       "max             NaN          NaN      NaN       NaN     2.300000e+01   \n",
       "\n",
       "        impression_dayofweek  \n",
       "count           3.845798e+06  \n",
       "unique                   NaN  \n",
       "top                      NaN  \n",
       "freq                     NaN  \n",
       "mean            2.886486e+00  \n",
       "std             1.977513e+00  \n",
       "min             0.000000e+00  \n",
       "25%             1.000000e+00  \n",
       "50%             3.000000e+00  \n",
       "75%             5.000000e+00  \n",
       "max             6.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>placement_id</th>\n",
       "      <th>cnxn_type</th>\n",
       "      <th>dma</th>\n",
       "      <th>country</th>\n",
       "      <th>prizm_premier_code</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>ua_browser</th>\n",
       "      <th>ua_os</th>\n",
       "      <th>ua_device_family</th>\n",
       "      <th>ua_device_brand</th>\n",
       "      <th>ua_is_mobile</th>\n",
       "      <th>ua_is_tablet</th>\n",
       "      <th>ua_is_pc</th>\n",
       "      <th>ua_is_bot</th>\n",
       "      <th>impression_hour</th>\n",
       "      <th>impression_dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557650</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>602</td>\n",
       "      <td>us</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9317</td>\n",
       "      <td>Mobile Safari UI/WKWebView</td>\n",
       "      <td>iOS</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>557650</td>\n",
       "      <td>Cable/DSL</td>\n",
       "      <td>623</td>\n",
       "      <td>us</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9317</td>\n",
       "      <td>Podcasts</td>\n",
       "      <td>iOS</td>\n",
       "      <td>iOS-Device</td>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>557650</td>\n",
       "      <td>Cable/DSL</td>\n",
       "      <td>602</td>\n",
       "      <td>us</td>\n",
       "      <td>21</td>\n",
       "      <td>9317</td>\n",
       "      <td>Podcasts</td>\n",
       "      <td>iOS</td>\n",
       "      <td>iOS-Device</td>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557650</td>\n",
       "      <td>Cable/DSL</td>\n",
       "      <td>602</td>\n",
       "      <td>us</td>\n",
       "      <td>21</td>\n",
       "      <td>9317</td>\n",
       "      <td>Podcasts</td>\n",
       "      <td>iOS</td>\n",
       "      <td>iOS-Device</td>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>557650</td>\n",
       "      <td>Cellular</td>\n",
       "      <td>602</td>\n",
       "      <td>us</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9317</td>\n",
       "      <td>Mobile Safari UI/WKWebView</td>\n",
       "      <td>iOS</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Apple</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   placement_id  cnxn_type  dma country prizm_premier_code  campaign_id  \\\n",
       "0        557650  Corporate  602      us            Unknown         9317   \n",
       "1        557650  Cable/DSL  623      us            Unknown         9317   \n",
       "2        557650  Cable/DSL  602      us                 21         9317   \n",
       "3        557650  Cable/DSL  602      us                 21         9317   \n",
       "4        557650   Cellular  602      us            Unknown         9317   \n",
       "\n",
       "                   ua_browser ua_os ua_device_family ua_device_brand  \\\n",
       "0  Mobile Safari UI/WKWebView   iOS           iPhone           Apple   \n",
       "1                    Podcasts   iOS       iOS-Device           Apple   \n",
       "2                    Podcasts   iOS       iOS-Device           Apple   \n",
       "3                    Podcasts   iOS       iOS-Device           Apple   \n",
       "4  Mobile Safari UI/WKWebView   iOS           iPhone           Apple   \n",
       "\n",
       "   ua_is_mobile  ua_is_tablet  ua_is_pc  ua_is_bot  impression_hour  \\\n",
       "0          True         False     False      False               14   \n",
       "1          True         False     False      False               18   \n",
       "2          True         False     False      False               11   \n",
       "3          True         False     False      False                3   \n",
       "4          True         False     False      False               18   \n",
       "\n",
       "   impression_dayofweek  \n",
       "0                     3  \n",
       "1                     3  \n",
       "2                     3  \n",
       "3                     3  \n",
       "4                     3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features.info()\n",
    "display(dataset.features.describe(include='all'))\n",
    "dataset.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: {'placement_id': 596772, 'cnxn_type': 'Corporate', 'dma': 602, 'country': 'us', 'prizm_premier_code': 'Unknown', 'campaign_id': 9317, 'ua_browser': 'Podcasts', 'ua_os': 'iOS', 'ua_device_family': 'iOS-Device', 'ua_device_brand': 'Apple', 'ua_is_mobile': True, 'ua_is_tablet': False, 'ua_is_pc': False, 'ua_is_bot': False, 'impression_hour': 10, 'impression_dayofweek': 0}\n",
      "Target: 0.0\n"
     ]
    }
   ],
   "source": [
    "# The class implements __getitem__ so that we can use indexing to get a single sample from the dataset.\n",
    "\n",
    "features, target = dataset[3000000]\n",
    "print(f\"Features: {features}\")\n",
    "print(f\"Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple neural network\n",
    "Now that we have the dataset and our features in a nice format, we can train a simple neural network.\n",
    "We do need to do some more playing around with the data though.\n",
    "\n",
    "1. The categorical features need to be converted to numerical values. I haven't thought through the \"best\" way to do this, but I will proceed with an encoding scheme for now. \n",
    "\n",
    "2. Then we also have the datetime features (day of week, hour of day) that need to be converted to numerical values. It makes sense to apply a sin/cosine transformation to these features. (23 hours is close to 0 for example).\n",
    "\n",
    "3. The numerical features should be scaled to a range of 0-1.\n",
    "\n",
    "4. The boolean features should be converted to 0/1 values. \n",
    "\n",
    "After this we can proceed with training a simple neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes:\n",
      "  Features: (3845798, 16)\n",
      "  Targets: torch.Size([3845798])\n",
      "\n",
      "Split shapes:\n",
      "  Train: X=(2692058, 16), y=torch.Size([2692058]) (2692058 samples)\n",
      "  Validation: X=(576870, 16), y=torch.Size([576870]) (576870 samples)\n",
      "  Test: X=(576870, 16), y=torch.Size([576870]) (576870 samples)\n",
      "\n",
      "Target variable proportions (Conversion Rate):\n",
      "  Original: 0.0072\n",
      "  Train: 0.0072\n",
      "  Validation: 0.0072\n",
      "  Test: 0.0072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Split the data into training and validation sets\n",
    "\n",
    "features_df = dataset.features\n",
    "targets_tensor = dataset.target\n",
    "\n",
    "targets_np = targets_tensor.cpu().numpy()\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "TRAIN_SIZE = 1.0 - TEST_SIZE - VAL_SIZE\n",
    "\n",
    "num_samples = len(features_df)\n",
    "indices = list(range(num_samples))\n",
    "\n",
    "train_val_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=targets_np  # https://scikit-learn.org/stable/modules/cross_validation.html#stratification\n",
    ")\n",
    "\n",
    "val_size = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_val_indices,\n",
    "    test_size=val_size,\n",
    "    random_state=42,\n",
    "    stratify=targets_np[train_val_indices]\n",
    ")\n",
    "\n",
    "X_train = features_df.iloc[train_indices].copy()\n",
    "y_train = targets_tensor[train_indices]\n",
    "\n",
    "X_val = features_df.iloc[val_indices].copy()\n",
    "y_val = targets_tensor[val_indices]\n",
    "\n",
    "X_test = features_df.iloc[test_indices].copy()\n",
    "y_test = targets_tensor[test_indices]\n",
    "\n",
    "# --- Print shapes to verify ---\n",
    "print(\"Original shapes:\")\n",
    "print(f\"  Features: {features_df.shape}\")\n",
    "print(f\"  Targets: {targets_tensor.shape}\")\n",
    "\n",
    "print(\"\\nSplit shapes:\")\n",
    "print(f\"  Train: X={X_train.shape}, y={y_train.shape} ({len(train_indices)} samples)\")\n",
    "print(f\"  Validation: X={X_val.shape}, y={y_val.shape} ({len(val_indices)} samples)\")\n",
    "print(f\"  Test: X={X_test.shape}, y={y_test.shape} ({len(test_indices)} samples)\")\n",
    "\n",
    "# --- Check stratification (optional, but good practice) ---\n",
    "# Calculate target proportions (conversion rate) in each set\n",
    "original_prop = targets_np.mean()\n",
    "train_prop = y_train.float().mean().item() # Use .float() for mean and .item() to get Python number\n",
    "val_prop = y_val.float().mean().item()\n",
    "test_prop = y_test.float().mean().item()\n",
    "\n",
    "print(\"\\nTarget variable proportions (Conversion Rate):\")\n",
    "print(f\"  Original: {original_prop:.4f}\")\n",
    "print(f\"  Train: {train_prop:.4f}\")\n",
    "print(f\"  Validation: {val_prop:.4f}\")\n",
    "print(f\"  Test: {test_prop:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make everything numeric\n",
    "Now that we have our train, test, validation sets, we can proceed with making everything numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting categorical encoder...\n",
      "Category mappings fitted. Example sizes (including unknown):\n",
      "  'placement_id': 4 unique values\n",
      "  'cnxn_type': 4 unique values\n",
      "  'dma': 179 unique values\n",
      "  'country': 47 unique values\n",
      "  'prizm_premier_code': 70 unique values\n",
      "  'campaign_id': 2 unique values\n",
      "  'ua_browser': 116 unique values\n",
      "  'ua_os': 18 unique values\n",
      "  'ua_device_family': 830 unique values\n",
      "  'ua_device_brand': 30 unique values\n",
      "\n",
      "Preparing numerical features for scaling...\n",
      "Numerical columns to scale: ['ua_is_mobile', 'ua_is_tablet', 'ua_is_pc', 'ua_is_bot', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos']\n",
      "Fitting numerical scaler...\n",
      "Numerical scaler fitted.\n",
      "\n",
      "Preprocessors (encoder, scaler, category sizes, feature lists) saved to './preprocessors'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "import joblib\n",
    "\n",
    "\n",
    "categorical_features = [\n",
    "    'placement_id', 'cnxn_type', 'dma', 'country', 'prizm_premier_code',\n",
    "    'campaign_id', 'ua_browser', 'ua_os', 'ua_device_family', 'ua_device_brand'\n",
    "]\n",
    "\n",
    "boolean_features = [\n",
    "    'ua_is_mobile', 'ua_is_tablet', 'ua_is_pc', 'ua_is_bot'\n",
    "]\n",
    "\n",
    "cyclical_features = [\n",
    "    'impression_hour', 'impression_dayofweek'\n",
    "]\n",
    "\n",
    "all_features = categorical_features + boolean_features + cyclical_features\n",
    "assert set(all_features) == set(X_train.columns), \"Features do not match\"\n",
    "\n",
    "print(\"\\nFitting categorical encoder...\")\n",
    "categorical_encoder = OrdinalEncoder(\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1, # Use -1 for unknown, easier to map later if needed\n",
    "    dtype=np.int64 # Ensure integer output\n",
    ")\n",
    "categorical_encoder.fit(X_train[categorical_features])\n",
    "\n",
    "\n",
    "category_sizes = {\n",
    "    col: len(cats) + 1 # Add 1 for the 'unknown' category we'll map to index 0\n",
    "    for col, cats in zip(categorical_features, categorical_encoder.categories_)\n",
    "}\n",
    "print(\"Category mappings fitted. Example sizes (including unknown):\")\n",
    "for i, (col, size) in enumerate(category_sizes.items()):\n",
    "    print(f\"  '{col}': {size} unique values\")\n",
    "\n",
    "\n",
    "print(\"\\nPreparing numerical features for scaling...\")\n",
    "temp_numeric_df = pd.DataFrame(index=X_train.index)\n",
    "\n",
    "# Convert booleans to float\n",
    "for col in boolean_features:\n",
    "    temp_numeric_df[col] = X_train[col].astype(float)\n",
    "\n",
    "# Apply cyclical transformations\n",
    "hour = X_train['impression_hour']\n",
    "day = X_train['impression_dayofweek']\n",
    "temp_numeric_df['hour_sin'] = np.sin(2 * np.pi * hour / 24.0)\n",
    "temp_numeric_df['hour_cos'] = np.cos(2 * np.pi * hour / 24.0)\n",
    "temp_numeric_df['day_sin'] = np.sin(2 * np.pi * day / 7.0)\n",
    "temp_numeric_df['day_cos'] = np.cos(2 * np.pi * day / 7.0)\n",
    "\n",
    "\n",
    "numerical_features_to_scale = temp_numeric_df.columns.tolist()\n",
    "print(f\"Numerical columns to scale: {numerical_features_to_scale}\")\n",
    "\n",
    "print(\"Fitting numerical scaler...\")\n",
    "numerical_scaler = StandardScaler()\n",
    "numerical_scaler.fit(temp_numeric_df[numerical_features_to_scale])\n",
    "print(\"Numerical scaler fitted.\")\n",
    "\n",
    "\n",
    "preprocessor_dir = './preprocessors' # Create this directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(preprocessor_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(categorical_encoder, os.path.join(preprocessor_dir, 'categorical_encoder.joblib'))\n",
    "joblib.dump(numerical_scaler, os.path.join(preprocessor_dir, 'numerical_scaler.joblib'))\n",
    "joblib.dump(category_sizes, os.path.join(preprocessor_dir, 'category_sizes.joblib'))\n",
    "joblib.dump(categorical_features, os.path.join(preprocessor_dir, 'categorical_features.joblib'))\n",
    "joblib.dump(boolean_features, os.path.join(preprocessor_dir, 'boolean_features.joblib'))\n",
    "joblib.dump(cyclical_features, os.path.join(preprocessor_dir, 'cyclical_features.joblib'))\n",
    "joblib.dump(numerical_features_to_scale, os.path.join(preprocessor_dir, 'numerical_features_to_scale.joblib'))\n",
    "\n",
    "print(f\"\\nPreprocessors (encoder, scaler, category sizes, feature lists) saved to '{preprocessor_dir}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- `categorical_encoder`: Fitted sklearn OrdinalEncoder.\n",
    "- `numerical_scaler`: Fitted sklearn StandardScaler.\n",
    "- `category_sizes`: Dictionary mapping categorical feature names to their vocabulary size (including unknown).\n",
    "- Lists of feature names for each type saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Error creating dataset. Could not read schema from './data/processed_splits/test_categorical_data.npy'. Is this a 'parquet' file?: Could not open Parquet input source './data/processed_splits/test_categorical_data.npy': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_processing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AuctionDataset\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset = AuctionDataset(dataframe=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data/processed_splits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pandas/io/parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1793\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1787\u001b[39m     warnings.warn(\n\u001b[32m   1788\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_legacy_dataset\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1790\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1813\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1371\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partitioning == \u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1368\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1369\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/dataset.py:794\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    783\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    784\u001b[39m     schema=schema,\n\u001b[32m    785\u001b[39m     filesystem=filesystem,\n\u001b[32m   (...)\u001b[39m\u001b[32m    790\u001b[39m     selector_ignore_prefixes=ignore_prefixes\n\u001b[32m    791\u001b[39m )\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/dataset.py:486\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    478\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    479\u001b[39m     partitioning=partitioning,\n\u001b[32m    480\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    481\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    482\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    483\u001b[39m )\n\u001b[32m    484\u001b[39m factory = FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3138\u001b[39m, in \u001b[36mpyarrow._dataset.DatasetFactory.finish\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/AuctioNN/.venv/lib/python3.13/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: Error creating dataset. Could not read schema from './data/processed_splits/test_categorical_data.npy'. Is this a 'parquet' file?: Could not open Parquet input source './data/processed_splits/test_categorical_data.npy': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.data_processing.datasets import AuctionDataset\n",
    "\n",
    "dataset = AuctionDataset(dataframe=pd.read_parquet('./data/'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AuctioNN (venv)",
   "language": "python",
   "name": "auctionn-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
